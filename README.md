## MNIST NN with two hidden layers (No TensorFlow)
### author: @zanolablue

Simple MNIST NN inspired by Samson Zhang video with updated normalization and an added hidden layer using the ReLU activation function, improving accuracy.
Below are the equations for the propagations:

**Forward propagation**

* ğ‘[1]=ğ‘Š[1]ğ‘‹+ğ‘[1]
  
* ğ´[1]=ğ‘”ReLU(ğ‘[1]))
  
* ğ‘[2]=ğ‘Š[2]ğ´[1]+ğ‘[2]
 
* ğ´[2]=ğ‘”ReLU(ğ‘[2]))
 
* ğ‘[3]=ğ‘Š[3]ğ´[2]+ğ‘[3]
 
* ğ´[3]=ğ‘”softmax(ğ‘[3])
  
**Backward propagation**

* ğ‘‘ğ‘[3]=ğ´[3]âˆ’ğ‘Œ
  
* ğ‘‘ğ‘Š[3]=1ğ‘šğ‘‘ğ‘[3]ğ´[2]ğ‘‡
  
* ğ‘‘ğµ[3]=1ğ‘šÎ£ğ‘‘ğ‘[3]
 
* ğ‘‘ğ‘[2]=ğ‘Š[3]ğ‘‡ğ‘‘ğ‘[3].âˆ—ğ‘”[1]â€²(ğ‘§[2])
  
* ğ‘‘ğ‘Š[2]=1ğ‘šğ‘‘ğ‘[2]ğ´[1]ğ‘‡
  
* ğ‘‘ğµ[2]=1ğ‘šÎ£ğ‘‘ğ‘[2]
  
* ğ‘‘ğ‘[1]=ğ‘Š[2]ğ‘‡ğ‘‘ğ‘[2].âˆ—ğ‘”[1]â€²(ğ‘§[1])
  
* ğ‘‘ğ‘Š[1]=1ğ‘šğ‘‘ğ‘[1]ğ´[0]ğ‘‡
  
* ğ‘‘ğµ[1]=1ğ‘šÎ£ğ‘‘ğ‘[1]
 
**Parameter updates**

* ğ‘Š[3]:=ğ‘Š[3]âˆ’ğ›¼ğ‘‘ğ‘Š[3]

* ğ‘[3]:=ğ‘[3]âˆ’ğ›¼ğ‘‘ğ‘[3]
 
* ğ‘Š[2]:=ğ‘Š[2]âˆ’ğ›¼ğ‘‘ğ‘Š[2]
 
* ğ‘[2]:=ğ‘[2]âˆ’ğ›¼ğ‘‘ğ‘[2]
  
* ğ‘Š[1]:=ğ‘Š[1]âˆ’ğ›¼ğ‘‘ğ‘Š[1]
  
* ğ‘[1]:=ğ‘[1]âˆ’ğ›¼ğ‘‘ğ‘[1]

Where the activation function ReLU() is being used for the hidden layers and softmax() is used to activate the output layer denoted by A[3]
