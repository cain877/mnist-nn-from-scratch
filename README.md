## MNIST NN with two hidden layers (No TensorFlow)
### author: @zanolablue

Simple MNIST NN inspired by Samson Zhang video with updated normalization and added a hidden layer using the ReLU activation function, improving accuracy.
Below are the equations for the propagations and activation functions:

**Forward propagation**

* ğ‘[1]=ğ‘Š[1]ğ‘‹+ğ‘[1]
  
* ğ´[1]=ğ‘”ReLU(ğ‘[1]))
  
* ğ‘[2]=ğ‘Š[2]ğ´[1]+ğ‘[2]
 
* ğ´[2]=ğ‘”ReLU(ğ‘[2]))
 
* ğ‘[3]=ğ‘Š[3]ğ´[2]+ğ‘[3]
 
* ğ´[3]=ğ‘”softmax(ğ‘[3])
  
**Backward propagation**

* ğ‘‘ğ‘[3]=ğ´[3]âˆ’ğ‘Œ
  
* ğ‘‘ğ‘Š[3]=1ğ‘šğ‘‘ğ‘[3]ğ´[2]ğ‘‡
  
* ğ‘‘ğµ[3]=1ğ‘šÎ£ğ‘‘ğ‘[3]
 
* ğ‘‘ğ‘[2]=ğ‘Š[3]ğ‘‡ğ‘‘ğ‘[3].âˆ—ğ‘”[1]â€²(ğ‘§[2])
  
* ğ‘‘ğ‘Š[2]=1ğ‘šğ‘‘ğ‘[2]ğ´[1]ğ‘‡
  
* ğ‘‘ğµ[2]=1ğ‘šÎ£ğ‘‘ğ‘[2]
  
* ğ‘‘ğ‘[1]=ğ‘Š[2]ğ‘‡ğ‘‘ğ‘[2].âˆ—ğ‘”[1]â€²(ğ‘§[1])
  
* ğ‘‘ğ‘Š[1]=1ğ‘šğ‘‘ğ‘[1]ğ´[0]ğ‘‡
  
* ğ‘‘ğµ[1]=1ğ‘šÎ£ğ‘‘ğ‘[1]
 
**Parameter updates**

* ğ‘Š[3]:=ğ‘Š[3]âˆ’ğ›¼ğ‘‘ğ‘Š[3]

* ğ‘[3]:=ğ‘[3]âˆ’ğ›¼ğ‘‘ğ‘[3]
 
* ğ‘Š[2]:=ğ‘Š[2]âˆ’ğ›¼ğ‘‘ğ‘Š[2]
 
* ğ‘[2]:=ğ‘[2]âˆ’ğ›¼ğ‘‘ğ‘[2]
  
* ğ‘Š[1]:=ğ‘Š[1]âˆ’ğ›¼ğ‘‘ğ‘Š[1]
  
* ğ‘[1]:=ğ‘[1]âˆ’ğ›¼ğ‘‘ğ‘[1]


