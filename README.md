## MNIST NN with two hidden layers (No TensorFlow)
### Author: Archie Galbraith <github.com/zanolablue>

Simple MNIST NN inspired by Samson Zhang with updated normalization in init_params and an added hidden layer, improving the overall accuracy of the original NN.

Below are the equations for the propagations where alpha is the learning rate (default 0.1), the activation function ReLU() is used for the hidden layers, and softmax() is used to activate the output layer denoted by A[3]:

**Forward propagation**

* ğ‘[1]=ğ‘Š[1]ğ‘‹+ğ‘[1]
  
* ğ´[1]=ğ‘”ReLU(ğ‘[1]))
  
* ğ‘[2]=ğ‘Š[2]ğ´[1]+ğ‘[2]
 
* ğ´[2]=ğ‘”ReLU(ğ‘[2]))
 
* ğ‘[3]=ğ‘Š[3]ğ´[2]+ğ‘[3]
 
* ğ´[3]=ğ‘”softmax(ğ‘[3])
  
**Backward propagation**

* ğ‘‘ğ‘[3]=ğ´[3]âˆ’ğ‘Œ
  
* ğ‘‘ğ‘Š[3]=1ğ‘šğ‘‘ğ‘[3]ğ´[2]ğ‘‡
  
* ğ‘‘ğµ[3]=1ğ‘šÎ£ğ‘‘ğ‘[3]
 
* ğ‘‘ğ‘[2]=ğ‘Š[3]ğ‘‡ğ‘‘ğ‘[3].âˆ—ğ‘”[1]â€²(ğ‘§[2])
  
* ğ‘‘ğ‘Š[2]=1ğ‘šğ‘‘ğ‘[2]ğ´[1]ğ‘‡
  
* ğ‘‘ğµ[2]=1ğ‘šÎ£ğ‘‘ğ‘[2]
  
* ğ‘‘ğ‘[1]=ğ‘Š[2]ğ‘‡ğ‘‘ğ‘[2].âˆ—ğ‘”[1]â€²(ğ‘§[1])
  
* ğ‘‘ğ‘Š[1]=1ğ‘šğ‘‘ğ‘[1]ğ´[0]ğ‘‡
  
* ğ‘‘ğµ[1]=1ğ‘šÎ£ğ‘‘ğ‘[1]
 
**Parameter updates**

* ğ‘Š[3]:=ğ‘Š[3]âˆ’ğ›¼ğ‘‘ğ‘Š[3]

* ğ‘[3]:=ğ‘[3]âˆ’ğ›¼ğ‘‘ğ‘[3]
 
* ğ‘Š[2]:=ğ‘Š[2]âˆ’ğ›¼ğ‘‘ğ‘Š[2]
 
* ğ‘[2]:=ğ‘[2]âˆ’ğ›¼ğ‘‘ğ‘[2]
  
* ğ‘Š[1]:=ğ‘Š[1]âˆ’ğ›¼ğ‘‘ğ‘Š[1]
  
* ğ‘[1]:=ğ‘[1]âˆ’ğ›¼ğ‘‘ğ‘[1]
